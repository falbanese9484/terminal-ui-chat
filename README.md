# Terminal Chat

Terminal UI for chatting with LLM models.

## Plan
1. Start with streaming, and try to plan a structure that gives us a stream bus that will be used
to feed the output to bubbletea for our eventual TUI.
2. Grab the bubbletea library and start building the console.
3. Change from the llama structure to a dynamic OpenRouter platform.
